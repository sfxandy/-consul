1. Basic Deployment

    GIVEN a Podman environment with at least 3 hosts available
    AND the latest Consul container image is accessible
    WHEN the deployment automation (script, Ansible, etc.) is executed
    THEN a 3-node Consul server cluster is started, with each node running as a Podman container on a separate host

2. Cluster Health Verification

    GIVEN the Consul containers are running
    WHEN the consul members command is executed on any node
    THEN all 3 nodes report as healthy, in the “server” role, and part of the same cluster

3. Data Persistence

    GIVEN Consul containers are configured with mapped volumes for data
    WHEN a container is stopped and restarted
    THEN its data and cluster state are preserved (no data loss, no rejoining as a new node)

4. Port and Network Accessibility

    GIVEN the cluster is running
    WHEN a request is sent to the Consul HTTP API (/v1/status/leader) from any host on the network
    THEN the API responds with the current cluster leader’s address

5. Security (Optional/Advanced)

    GIVEN Consul is started with ACLs and TLS enabled
    WHEN an unauthorized or non-TLS request is made to the API
    THEN the request is rejected with an appropriate error

6. Idempotent Re-deployment

    GIVEN the deployment automation is run a second time
    WHEN no configuration has changed
    THEN no new containers are created, existing ones are verified as healthy, and cluster state is maintained

7. Log and Monitoring Integration

    GIVEN the containers are running
    WHEN logs are collected from the Consul containers
    THEN log messages are available for health, cluster join/leave, and error events, and can be accessed via the centralized logging solution


---------------------------------------------------------


    Given the cluster has no existing Consul server instances,
    when the deployment automation is executed,
    then the first server is initialized as the bootstrap node and additional servers join automatically as peers.

Service Registration

    Given a Consul server cluster is running,
    when a sample service is registered via the HTTP API from any node,
    then the service appears in the Consul UI and can be discovered by other nodes.

Consul UI Accessibility

    Given the cluster is running,
    when the Consul UI port (default 8500) is accessed from a browser on the admin network,
    then the Consul web interface loads and shows cluster health and registered services.

Failure Recovery

    Given all Consul server containers are running,
    when one container is forcefully stopped or removed,
    then the remaining servers maintain quorum and cluster leadership is re-established automatically.

Configuration Management

    Given the deployment automation supports configuration templating,
    when a configuration change (e.g., enable/disable specific Consul features) is applied and redeployed,
    then the cluster applies the new settings with minimal downtime and configuration drift is avoided.

Network Partition Handling

    Given a network partition occurs that isolates one or more Consul nodes,
    when connectivity is restored,
    then all nodes rejoin the cluster automatically without manual intervention.

Resource Limits and Health Checks

    Given Consul containers have resource constraints (CPU, memory) and health checks configured,
    when a resource limit is reached or a health check fails,
    then alerts are generated and the unhealthy container is restarted according to policy.

Rolling Upgrade

    Given the cluster is running a previous Consul version,
    when the deployment automation upgrades each node to a new version one at
